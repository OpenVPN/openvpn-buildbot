# -*- python -*-
# ex: set filetype=python:

import configparser
from datetime import timedelta
import json
import treq
import os
import random
import re
import time

from twisted.internet import defer
from twisted.logger import Logger
from twisted.python import failure

from urllib.parse import urlparse

from buildbot.plugins import *
from buildbot.plugins import reporters, secrets, util
from buildbot.process.results import SUCCESS, WARNINGS
from buildbot.process.properties import Interpolate
from buildbot.process.properties import Properties
from buildbot.reporters.base import ReporterBase
from buildbot.reporters.generators.build import BuildStartEndStatusGenerator
from buildbot.reporters.message import MessageFormatterRenderable
from buildbot.util import httpclientservice
from buildbot.util.ssfilter import SourceStampFilter

############################################################################
### Copy of Gerrit code from buildbot 3.11.x since 4.x version is broken ###
### cf. https://github.com/buildbot/buildbot/issues/8711                 ###
############################################################################

# This file is part of Buildbot.  Buildbot is free software: you can
# redistribute it and/or modify it under the terms of the GNU General Public
# License as published by the Free Software Foundation, version 2.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
# details.
#
# You should have received a copy of the GNU General Public License along with
# this program; if not, write to the Free Software Foundation, Inc., 51
# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#
# Copyright Buildbot Team Members

import copy
import datetime
import json

from twisted.python import log

from buildbot import config
from buildbot.changes import base
from buildbot.config.checks import check_param_int_none
from buildbot.util import bytes2unicode
from buildbot.util import httpclientservice
from buildbot.util import runprocess
from buildbot.util.pullrequest import PullRequestMixin


def _canonicalize_event(event):
    """
    Return an event dictionary which is consistent between the gerrit
    event stream and the gerrit event log formats.
    """
    # For "patchset-created" the events-log JSON looks like:
    #   "project": {"name": "buildbot"}
    # while the stream-events JSON looks like:
    #   "project": "buildbot"
    # so we canonicalize them to the latter
    if "change" not in event:
        return event

    change = event["change"]
    if "project" not in change:
        return event

    project = change["project"]
    if not isinstance(project, dict):
        return event

    if "name" not in project:
        return event

    event = copy.deepcopy(event)
    event["change"]["project"] = project["name"]
    return event


def _gerrit_user_to_author(props, username="unknown"):
    """
    Convert Gerrit account properties to Buildbot format

    Take into account missing values
    """
    username = props.get("username", username)
    username = props.get("name", username)
    if "email" in props:
        username += f" <{props['email']}>"
    return username


class OurGerritChangeSourceBase(base.ChangeSource, PullRequestMixin):
    """This source will maintain a connection to gerrit ssh server
    that will provide us gerrit events in json format."""

    compare_attrs = ("gerritserver", "gerritport")
    name = None
    # list of properties that are no of no use to be put in the event dict
    external_property_denylist = ["event.eventCreatedOn"]
    external_property_whitelist = ["*"]
    property_basename = "event"

    def checkConfig(
        self,
        gitBaseURL=None,
        handled_events=("patchset-created", "ref-updated"),
        debug=False,
        get_files=False,
    ):

        if gitBaseURL is None:
            config.error("gitBaseURL must be specified")

    def reconfigService(
        self,
        gitBaseURL=None,
        handled_events=("patchset-created", "ref-updated"),
        debug=False,
        get_files=False,
    ):
        self.gitBaseURL = gitBaseURL
        self.handled_events = list(handled_events)
        self._get_files = get_files
        self.debug = debug

    def lineReceived(self, line):
        try:
            event = json.loads(bytes2unicode(line))
        except ValueError:
            log.msg(f"bad json line: {line}")
            return defer.succeed(None)

        if not (isinstance(event, dict) and "type" in event):
            if self.debug:
                log.msg(f"no type in event {line}")
            return defer.succeed(None)

        return self.eventReceived(event)

    def build_properties(self, event):
        properties = self.extractProperties(event)
        properties["event.source"] = self.__class__.__name__
        if event["type"] in ("patchset-created", "comment-added") and "change" in event:
            properties["target_branch"] = event["change"]["branch"]
        return properties

    def eventReceived(self, event):
        if not event["type"] in self.handled_events:
            if self.debug:
                log.msg(f"the event type '{event['type']}' is not setup to handle")
            return defer.succeed(None)

        properties = self.build_properties(event)
        func_name = f'eventReceived_{event["type"].replace("-", "_")}'
        func = getattr(self, func_name, None)
        if func is None:
            return self.addChangeFromEvent(properties, event)

        return func(properties, event)

    @defer.inlineCallbacks
    def addChange(self, event_type, chdict):
        stampdict = {
            "branch": chdict["branch"],
            "revision": chdict["revision"],
            "patch_author": chdict["author"],
            "patch_comment": chdict["comments"],
            "repository": chdict["repository"],
            "project": chdict["project"],
            "codebase": "",
        }

        _, found_existing = yield self.master.db.sourcestamps.findOrCreateId(
            **stampdict
        )

        if found_existing and event_type in ("patchset-created", "ref-updated"):
            if self.debug:
                eventstr = (
                    f'{self.gitBaseURL}/{chdict["project"]} -- '
                    f'{chdict["branch"]}:{chdict["revision"]}'
                )
                message = f"gerrit: duplicate change event {eventstr} by {self.__class__.__name__}"
                log.msg(message.encode("utf-8"))
            return

        if self.debug:
            eventstr = (
                f'{chdict["repository"]} -- {chdict["branch"]}:{chdict["revision"]}'
            )
            message = (
                f"gerrit: adding change from {eventstr} in {self.__class__.__name__}"
            )
            log.msg(message.encode("utf-8"))

        try:
            yield self.master.data.updates.addChange(**chdict)
        except Exception:
            # eat failures..
            log.err("error adding change from GerritChangeSource")

    def get_branch_from_event(self, event):
        if event["type"] in ("patchset-created", "comment-added"):
            return event["patchSet"]["ref"]
        return event["change"]["branch"]

    def strip_refs_heads_from_branch(self, branch):
        if branch.startswith("refs/heads/"):
            branch = branch[len("refs/heads/") :]
        return branch

    @defer.inlineCallbacks
    def addChangeFromEvent(self, properties, event):
        if "change" not in event:
            if self.debug:
                log.msg(f'unsupported event {event["type"]}')
            return None

        if "patchSet" not in event:
            if self.debug:
                log.msg(f'unsupported event {event["type"]}')
            return None

        event = _canonicalize_event(event)
        event_change = event["change"]

        files = ["unknown"]
        if self._get_files:
            files = yield self.getFiles(
                change=event_change["number"], patchset=event["patchSet"]["number"]
            )

        yield self.addChange(
            event["type"],
            {
                "author": _gerrit_user_to_author(event_change["owner"]),
                "project": bytes2unicode(event_change["project"]),
                "repository": f'{self.gitBaseURL}/{event_change["project"]}',
                "branch": self.get_branch_from_event(event),
                "revision": event["patchSet"]["revision"],
                "revlink": event_change["url"],
                "comments": event_change["subject"],
                "files": files,
                "category": event["type"],
                "properties": properties,
            },
        )
        return None

    def eventReceived_ref_updated(self, properties, event):
        ref = event["refUpdate"]
        author = "gerrit"

        if "submitter" in event:
            author = _gerrit_user_to_author(event["submitter"], author)

        # Ignore ref-updated events if patchset-created events are expected for this push.
        # ref-updated events may arrive before patchset-created events and cause problems, as
        # builds would be using properties from ref-updated event and not from patchset-created.
        # As a result it may appear that the change was not related to a Gerrit change and cause
        # reporters to not submit reviews for example.
        if "patchset-created" in self.handled_events and ref["refName"].startswith(
            "refs/changes/"
        ):
            return None

        return self.addChange(
            event["type"],
            {
                "author": author,
                "project": ref["project"],
                "repository": f'{self.gitBaseURL}/{ref["project"]}',
                "branch": self.strip_refs_heads_from_branch(ref["refName"]),
                "revision": ref["newRev"],
                "comments": "Gerrit: commit(s) pushed.",
                "files": ["unknown"],
                "category": event["type"],
                "properties": properties,
            },
        )


class OurGerritEventLogPoller(OurGerritChangeSourceBase):

    POLL_INTERVAL_SEC = 30
    FIRST_FETCH_LOOKBACK_DAYS = 1

    def checkConfig(
        self,
        baseURL,
        auth,
        pollInterval=POLL_INTERVAL_SEC,
        pollAtLaunch=True,
        firstFetchLookback=FIRST_FETCH_LOOKBACK_DAYS,
        **kwargs,
    ):
        if self.name is None:
            self.name = f"GerritEventLogPoller:{baseURL}"
        super().checkConfig(**kwargs)

    @defer.inlineCallbacks
    def reconfigService(
        self,
        baseURL,
        auth,
        pollInterval=POLL_INTERVAL_SEC,
        pollAtLaunch=True,
        firstFetchLookback=FIRST_FETCH_LOOKBACK_DAYS,
        **kwargs,
    ):

        yield super().reconfigService(**kwargs)
        if baseURL.endswith("/"):
            baseURL = baseURL[:-1]

        self._pollInterval = pollInterval
        self._pollAtLaunch = pollAtLaunch
        self._oid = yield self.master.db.state.getObjectId(
            self.name, self.__class__.__name__
        )
        self._http = yield httpclientservice.HTTPClientService.getService(
            self.master, baseURL, auth=auth
        )

        self._first_fetch_lookback = firstFetchLookback
        self._last_event_time = None

    @staticmethod
    def now():
        """patchable now (datetime is not patchable as builtin)"""
        return datetime.datetime.now(datetime.timezone.utc)

    @defer.inlineCallbacks
    def poll(self):
        last_event_ts = yield self.master.db.state.getState(
            self._oid, "last_event_ts", None
        )
        if last_event_ts is None:
            # If there is not last event time stored in the database, then set
            # the last event time to some historical look-back
            last_event = self.now() - datetime.timedelta(
                days=self._first_fetch_lookback
            )
        else:
            last_event = datetime.datetime.fromtimestamp(
                last_event_ts, datetime.timezone.utc
            )
        last_event_formatted = last_event.strftime("%Y-%m-%d %H:%M:%S")

        if self.debug:
            log.msg(
                f"{self.name}: Polling gerrit: {last_event_formatted}".encode("utf-8")
            )

        res = yield self._http.get(
            "/plugins/events-log/events/", params={"t1": last_event_formatted}
        )
        lines = yield res.content()
        for line in lines.splitlines():
            yield self.lineReceived(line)

    @defer.inlineCallbacks
    def eventReceived(self, event):
        res = yield super().eventReceived(event)
        if "eventCreatedOn" in event:
            yield self.master.db.state.setState(
                self._oid, "last_event_ts", event["eventCreatedOn"]
            )
        return res

    @defer.inlineCallbacks
    def getFiles(self, change, patchset):
        res = yield self._http.get(f"/changes/{change}/revisions/{patchset}/files/")
        res = yield res.content()

        res = res.splitlines()[1].decode(
            "utf8"
        )  # the first line of every response is `)]}'`
        return list(json.loads(res))

    # FIXME this copy the code from PollingChangeSource
    # but as PollingChangeSource and its subclasses need to be ported to reconfigurability
    # we can't use it right now
    @base.poll_method
    def doPoll(self):
        d = defer.maybeDeferred(self.poll)
        d.addErrback(log.err, "while polling for changes")
        return d

    def force(self):
        self.doPoll()

    def activate(self):
        self.doPoll.start(interval=self._pollInterval, now=self._pollAtLaunch)

    def deactivate(self):
        return self.doPoll.stop()

    def describe(self):
        msg = "GerritEventLogPoller watching the remote " "Gerrit repository {}"
        return msg.format(self.name)


### End of Copy ###

logger = Logger()


class GerritChecks(ReporterBase):
    name = "GerritChecks"

    def checkConfig(
        self,
        baseURL,
        auth=None,
        verification_name=None,
        verbose=False,
        debug=None,
        verify=None,
        generators=None,
        **kwargs,
    ):
        if generators is None:
            generators = self._create_default_generators()

        super().checkConfig(generators=generators, **kwargs)
        httpclientservice.HTTPClientService.checkAvailable(self.__class__.__name__)

    @defer.inlineCallbacks
    def reconfigService(
        self,
        baseURL,
        auth=None,
        verification_name=None,
        verbose=False,
        debug=None,
        verify=None,
        generators=None,
        **kwargs,
    ):
        self.debug = debug
        self.verify = verify
        self.verbose = verbose

        if generators is None:
            generators = self._create_default_generators()

        yield super().reconfigService(generators=generators, **kwargs)

        if baseURL.endswith("/"):
            baseURL = baseURL[:-1]

        self._http = yield httpclientservice.HTTPClientService.getService(
            self.master, baseURL, auth=auth, debug=self.debug, verify=self.verify
        )

        self._verification_name = verification_name or Interpolate(
            "%(prop:buildername)s"
        )
        self._verbose = verbose

    def _create_default_generators(self):
        start_formatter = MessageFormatterRenderable("Build started.")
        end_formatter = MessageFormatterRenderable("Build done.")

        return [
            BuildStartEndStatusGenerator(
                start_formatter=start_formatter, end_formatter=end_formatter
            )
        ]

    def createStatus(self, change_id, revision_id, uuid, state, url=None):
        payload = {"checker_uuid": uuid, "state": state}

        if url is not None:
            payload["url"] = url

        if self._verbose:
            logger.debug(
                "Sending Gerrit status for {change_id}/{revision_id}: data={data}",
                change_id=change_id,
                revision_id=revision_id,
                data=payload,
            )

        return self._http.post(
            "/".join(
                ["/a/changes", str(change_id), "revisions", str(revision_id), "checks"]
            ),
            json=payload,
        )

    @staticmethod
    def getGerritChanges(props):
        if "gerrit_changes" in props:
            return props.getProperty("gerrit_changes")

        if "event.change.number" in props:
            return [
                {
                    "change_id": props.getProperty("event.change.number"),
                    "revision_id": props.getProperty("event.patchSet.number"),
                }
            ]
        return []

    @defer.inlineCallbacks
    def sendMessage(self, reports):
        build = reports[0]["builds"][0]

        props = Properties.fromDict(build["properties"])
        props.master = self.master

        if build["complete"]:
            if build["results"] == util.SUCCESS:
                state = "SUCCESSFUL"
            else:
                state = "FAILED"
        else:
            state = "RUNNING"

        name = yield props.render(self._verification_name)
        uuid = "buildbot:%s" % name.replace("=", "")

        changes = yield self.getGerritChanges(props)
        for change in changes:
            try:
                yield self.createStatus(
                    change["change_id"],
                    change["revision_id"],
                    uuid,
                    state,
                    url=build["url"],
                )
            except Exception:
                logger.failure("Failed to send status!", failure=failure.Failure())


def get_worker_setting(config, worker_name, setting):
    """Load a worker setting from worker.ini"""
    worker_config = config

    try:
        value = worker_config.get(worker_name, setting)
    except configparser.NoOptionError:
        value = None

    return value


def get_config(name):
    """Return active master configuration"""
    config = configparser.ConfigParser()

    custom = f"{name}.ini"
    default = f"{name}-default.ini"

    if os.path.exists(custom):
        config.read(custom)
    elif os.path.exists(default):
        config.read(default)
    else:
        print(f"ERROR: could not find {custom} or {default}")
        sys.exit(1)

    return config


# Load configuration from configuration files
worker_config = get_config("worker")
master_config = get_config("master")

c = BuildmasterConfig = {}

# Global settings
persistent_dir = os.path.expanduser(os.path.join(basedir, "persistent"))
master_fqdn = master_config.get("master", "master_fqdn")
database = master_config.get("master", "database")
if database == "default":
    database = f"sqlite:////{persistent_dir}/libstate.sqlite"
buildbot_url = master_config.get("master", "buildbot_url")
title_url = master_config.get("master", "title_url")
docker_network = master_config.get("docker", "network")
cancel_old_builds = master_config.getboolean("master", "cancel_old_builds")
have_ec2_workers = master_config.getboolean("master", "have_ec2_workers")
if have_ec2_workers:
    ec2_key = master_config.get("ec2", "key")
    ec2_region = master_config.get("ec2", "region")
    ec2_subnet = master_config.get("ec2", "subnet")
    ec2_sgs = json.loads(master_config.get("ec2", "security_groups"))
vcpkg_cache = master_config.get("vcpkg", "cache")

# Global email settings
notify_on_missing = json.loads(master_config.get("email", "notify_on_missing"))
extra_recipients = json.loads(master_config.get("email", "extra_recipients"))
fromaddr = master_config.get("email", "fromaddr")
relayhost = master_config.get("email", "relayhost")

# gerrit settings
gerrit_repo_url = master_config.get("gerrit", "repo_url")
gerrit_git_base_url = master_config.get("gerrit", "git_base_url")
verified_authors_list = json.loads(master_config.get("gerrit", "verified_authors_list"))
gerrit_user_password = master_config.get("gerrit", "user_password")

# openvpn settings
openvpn_owner = master_config.get("openvpn", "owner")
openvpn_repo_url = master_config.get("openvpn", "repo_url")
openvpn_main_branch = master_config.get("openvpn", "main_branch")
openvpn_release_branch = master_config.get("openvpn", "release_branch")
openvpn_run_tserver_null_tests = master_config.getboolean(
    "openvpn", "run_tserver_null_tests"
)
openvpn_run_windows_builds = master_config.getboolean("openvpn", "run_windows_builds")
openvpn_run_tclient_tests = master_config.getboolean("openvpn", "run_tclient_tests")
openvpn_run_code_check = master_config.getboolean("openvpn", "run_code_check")
otst = master_config.get("openvpn", "tree_stable_timer")
openvpn_tree_stable_timer = None if otst == "None" else int(otst)

# for packaging
openvpn_build_repo_url = master_config.get("openvpn-build", "repo_url")

# openvpn3 settings
openvpn3_owner = master_config.get("openvpn3", "owner")
openvpn3_repo_url = master_config.get("openvpn3", "repo_url")
openvpn3_branch = master_config.get("openvpn3", "branch")
o3tst = master_config.get("openvpn3", "tree_stable_timer")
openvpn3_tree_stable_timer = None if o3tst == "None" else int(o3tst)

# openvpn3-linux settings
openvpn3_linux_owner = master_config.get("openvpn3-linux", "owner")
openvpn3_linux_repo_url = master_config.get("openvpn3-linux", "repo_url")
openvpn3_linux_branch = master_config.get("openvpn3-linux", "branch")
o3ltst = master_config.get("openvpn3-linux", "tree_stable_timer")
openvpn3_linux_tree_stable_timer = None if o3ltst == "None" else int(o3ltst)

# ovpn-dco settings
ovpn_dco_owner = master_config.get("ovpn-dco", "owner")
ovpn_dco_repo_url = master_config.get("ovpn-dco", "repo_url")
ovpn_dco_branch = master_config.get("ovpn-dco", "branch")
odtst = master_config.get("ovpn-dco", "tree_stable_timer")
ovpn_dco_tree_stable_timer = None if odtst == "None" else int(odtst)
# Each section in worker.ini represents one buildbot worker. The "DEFAULT"
# section gives the default settings and gets filtered out automatically, i.e.
# does not become a worker.
worker_names = worker_config.sections()
# Create a list of builders for each scheduler we will create
builder_names = {}
scheduler_names = [
    "openvpn_main",
    "openvpn_release",
    "openvpn-smoketest",
    "openvpn3",
    "openvpn3-smoketest",
    "openvpn3-linux",
    "openvpn3-linux-smoketest",
    "ovpn-dco",
    "ovpn-dco-smoketest",
]
for scheduler in scheduler_names:
    builder_names[scheduler] = []

# Load configuration options used for packaging, connectivity testing and compile tests
build_and_test_config_opt_combos = json.loads(
    master_config.get("openvpn", "build_and_test_config_opt_combos", fallback="[]")
)
compile_config_opt_combos = json.loads(
    master_config.get("openvpn", "compile_config_opt_combos", fallback="[]")
)
packaging_config_opt_combos = json.loads(
    master_config.get("openvpn", "packaging_config_opt_combos", fallback="[]")
)

c["secretsProviders"] = [
    secrets.SecretInAFile(dirname=os.path.join(persistent_dir, "secrets"))
]

c["workers"] = []

# Create normal and latent workers
for worker_name in worker_names:
    worker_persist = worker_config.get(worker_name, "persist")
    if worker_config.get(worker_name, "type") == "latent_docker":
        image = worker_config.get(worker_name, "image")
        docker_url = worker_config.get(worker_name, "docker_url")
        master_fqdn = worker_config.get(worker_name, "master_fqdn")

        c["workers"].append(
            worker.DockerLatentWorker(
                worker_name,
                worker_config.get(worker_name, "password"),
                max_builds=2,
                notify_on_missing=notify_on_missing,
                properties={
                    "persist": worker_persist,
                },
                docker_host=docker_url,
                followStartupLogs=True,
                image=image,
                masterFQDN=master_fqdn,
                volumes=[
                    f"buildbot-worker-{worker_name}:{worker_persist}",
                    f"buildbot-worker-{worker_name}-build:/build",
                ],
                hostconfig={
                    "network_mode": docker_network,
                    "sysctls": {"net.ipv6.conf.all.disable_ipv6": 0},
                    "cap_add": ["NET_ADMIN"],
                    "devices": ["/dev/net/tun:/dev/net/tun"],
                    "mem_limit": "4g",
                },
            )
        )
        if worker_config.get(worker_name, "enable_tclient_builds") == "true":
            c["workers"].append(
                worker.DockerLatentWorker(
                    f"{worker_name}-tc",
                    worker_config.get(worker_name, "password"),
                    max_builds=1,
                    notify_on_missing=notify_on_missing,
                    properties={
                        "persist": worker_persist,
                    },
                    docker_host=docker_url,
                    followStartupLogs=True,
                    image=image,
                    masterFQDN=master_fqdn,
                    volumes=[
                        f"buildbot-worker-{worker_name}-tc:{worker_persist}",
                        f"buildbot-worker-{worker_name}-tc-build:/build",
                    ],
                    hostconfig={
                        "network_mode": docker_network,
                        "sysctls": {"net.ipv6.conf.all.disable_ipv6": 0},
                        "cap_add": ["NET_ADMIN"],
                        "devices": ["/dev/net/tun:/dev/net/tun"],
                        "mem_limit": "4g",
                    },
                )
            )
    elif worker_config.get(worker_name, "type") == "latent_ec2":
        image = worker_config.get(worker_name, "image")
        instance = worker_config.get(worker_name, "instance")
        profile = get_worker_setting(worker_config, worker_name, "iam_profile")

        c["workers"].append(
            worker.EC2LatentWorker(
                worker_name,
                worker_config.get(worker_name, "password"),
                instance,
                ami=image,
                region=ec2_region,
                subnet_id=ec2_subnet,
                security_group_ids=ec2_sgs,
                keypair_name=ec2_key,
                instance_profile_name=profile,
                max_builds=1,
                notify_on_missing=notify_on_missing,
                # properties={
                #     "persist": worker_persist,
                # },
                tags={
                    "Name": worker_name,
                    "owner": openvpn_owner,
                    "created-by": "buildbot",
                    "documentation": "https://github.com/OpenVPN/openvpn-buildbot",
                    "Login": "buildbot",
                },
            )
        )
    else:
        c["workers"].append(
            worker.Worker(
                worker_name,
                worker_config.get(worker_name, "password"),
                max_builds=1,
                notify_on_missing=notify_on_missing,
                properties={
                    "persist": worker_persist,
                },
            )
        )
        if worker_config.get(worker_name, "enable_separate_tclient_worker") == "true":
            c["workers"].append(
                worker.Worker(
                    f"{worker_name}-tc",
                    worker_config.get(worker_name, "password"),
                    max_builds=1,
                    notify_on_missing=notify_on_missing,
                    properties={
                        "persist": worker_persist,
                    },
                )
            )


c["protocols"] = {"pb": {"port": r"tcp:interface=\:\:0:port=9989"}}

c["change_source"] = []

# any gerrit repo (but for now only OpenVPN 2)
c["change_source"].append(
    OurGerritEventLogPoller(
        baseURL=gerrit_repo_url + "/a",
        auth=("buildbot", gerrit_user_password),
        gitBaseURL=gerrit_git_base_url,
        get_files=True,
        handled_events=["patchset-created"],
        debug=True,
    )
)

# OpenVPN 2 Git repository
c["change_source"].append(
    changes.GitPoller(
        repourl=openvpn_repo_url,
        branches=[openvpn_main_branch, openvpn_release_branch],
        project="openvpn",
        workdir="persistent/gitpoller-workdir-openvpn",
        pollInterval=60,
    )
)

# OpenVPN 3 Git repository
c["change_source"].append(
    changes.GitPoller(
        repourl=openvpn3_repo_url,
        branch=openvpn3_branch,
        project="openvpn3",
        workdir="persistent/gitpoller-workdir-openvpn3",
        pollInterval=60,
    )
)

c["change_source"].append(
    changes.GitPoller(
        repourl=openvpn3_linux_repo_url,
        branch=openvpn3_linux_branch,
        project="openvpn3-linux",
        workdir="persistent/gitpoller-workdir-openvpn3-linux",
        pollInterval=60,
    )
)

c["change_source"].append(
    changes.GitPoller(
        repourl=ovpn_dco_repo_url,
        branch=ovpn_dco_branch,
        project="ovpn-dco",
        workdir="persistent/gitpoller-workdir-ovpn-dco",
        pollInterval=60,
    )
)

c["builders"] = []

# Limit concurrent builds on each Docker host
max_builds = json.loads(master_config.get("docker", "max_builds"))
docker_build_locks = {}

for docker_host, maxcount in max_builds.items():
    docker_build_locks[docker_host] = util.MasterLock(docker_host, maxCount=maxcount)

# Only allow one docker worker to run t_client tests at the same time. This is
# convenience feature to reduce the number of keys required for t_client tests.
docker_tclient_lock = util.MasterLock("docker", maxCount=1)


def getBuilderNameSuffix(combo):
    """Generate builder name suffix from configure options"""
    if not combo:
        return "-default"
    else:
        return combo.replace(" ", "")


def getFactoryName(combo):
    return f"factory{getBuilderNameSuffix(combo)}"


# Load build steps from separate files
#
# OpenVPN 2 build steps
for steps_file in [
    "common_unix_steps.cfg",
    "cmake_unix_steps.cfg",
    "android_unix_steps.cfg",
    "common_windows_steps.cfg",
    "common_mingw_steps.cfg",
    "debian_packaging_steps.cfg",
    "doxygen_build_steps.cfg",
    "tserver_null_pre_steps.cfg",
    "tclient_pre_steps.cfg",
    "check_steps.cfg",
    "tclient_post_steps.cfg",
    "unix_compile_steps.cfg",
    "unix_unit-test_steps.cfg",
    "code_check.cfg",
]:
    exec(
        compile(
            source=open(os.path.join("openvpn", steps_file)).read(),
            filename=os.path.join("openvpn", steps_file),
            mode="exec",
        )
    )

# OpenVPN 3 build steps
for steps_file in ["common_linux_steps.cfg"]:
    exec(
        compile(
            source=open(os.path.join("openvpn3", steps_file)).read(),
            filename=os.path.join("openvpn3", steps_file),
            mode="exec",
        )
    )

# OpenVPN 3 linux build steps
for steps_file in ["common_linux_steps.cfg", "gdbuspp_steps.cfg"]:
    exec(
        compile(
            source=open(os.path.join("openvpn3-linux", steps_file)).read(),
            filename=os.path.join("openvpn3-linux", steps_file),
            mode="exec",
        )
    )

# ovpn-dco build steps
for steps_file in ["common_linux_steps.cfg"]:
    exec(
        compile(
            source=open(os.path.join("ovpn-dco", steps_file)).read(),
            filename=os.path.join("ovpn-dco", steps_file),
            mode="exec",
        )
    )

ccache = {
    "PATH": [
        "/usr/local/opt/ccache/libexec",
        "/usr/lib64/ccache",
        "/usr/lib/ccache/bin",
        "/usr/lib/ccache",
        "${PATH}",
    ],
    "CCACHE_DIR": util.Interpolate("%(prop:persist)s/ccache"),
    "CCACHE_MAXSIZE": "1Gi",
}

lwip_env = {"LWIPOVPN_PATH": "/buildbot/lwipovpnbuild/lwipovpn"}

vcpkg_env = {"VCPKG_BINARY_SOURCES": vcpkg_cache}

factories = {}

# OpenVPN 2 code check
if openvpn_run_code_check:
    factory = util.BuildFactory()
    factory = openvpnAddFormatStepsToBuildFactory(factory, ccache)
    factory_name = "openvpn-code-check"
    factories.update(
        {
            factory_name: {
                "factory": factory,
                "os": "unix",
                "types": ["code-check", "openvpn"],
                "schedulers": ["openvpn_main", "openvpn_release"],
            }
        }
    )
    del factory

# OpenVPN 2 doxygen build using default configure options
factory = util.BuildFactory()
factory = openvpnAddCommonUnixStepsToBuildFactory(factory, "", ccache)
factory = openvpnAddDoxygenStepsToBuildFactory(factory, ccache)
factory_name = "doxygen"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["doxygen", "openvpn"],
            "schedulers": ["openvpn_main"],
        }
    }
)
del factory

# OpenVPN 2 smoketest using default configure options
factory = util.BuildFactory()
factory = openvpnAddCommonUnixStepsToBuildFactory(factory, "", ccache)
factory = openvpnAddUnixCompileStepsToBuildFactory(factory, "", ccache)
factory = openvpnAddUnixUnitTestStepsToBuildFactory(factory, "", ccache)
factory_name = "smoketest"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openvpn-smoketest"],
            "schedulers": ["openvpn-smoketest"],
        }
    }
)
del factory

# Basic OpenVPN 2 compile tests on Unix-style operating systems
for combo in compile_config_opt_combos:
    factory = util.BuildFactory()
    factory = openvpnAddCommonUnixStepsToBuildFactory(factory, combo, ccache)
    factory = openvpnAddUnixCompileStepsToBuildFactory(factory, combo, ccache)
    factory = openvpnAddUnixUnitTestStepsToBuildFactory(factory, combo, ccache)
    factory_name = getFactoryName(combo)
    factories.update(
        {
            factory_name: {
                "factory": factory,
                "os": "unix",
                "types": ["openvpn", "compile"],
                "schedulers": ["openvpn_main", "openvpn_release"],
            }
        }
    )
    del factory

# OpenVPN 2 connectivity tests on Unix-style operating systems
if openvpn_run_tclient_tests or openvpn_run_tserver_null_tests:
    for combo in build_and_test_config_opt_combos:
        make_check_env = {}
        make_check_env.update(ccache)
        make_check_env.update(lwip_env)
        if openvpn_run_tclient_tests:
            make_check_env.update({"TCLIENT_SKIP_RC": "1"})

        factory = util.BuildFactory()
        factory = openvpnAddCommonUnixStepsToBuildFactory(
            factory, combo["configure"], make_check_env
        )
        factory = openvpnAddUnixCompileStepsToBuildFactory(
            factory, combo["configure"], make_check_env
        )

        if openvpn_run_tserver_null_tests:
            factory = openvpnAddTServerNullPreStepsToBuildFactory(
                factory, combo["configure"], make_check_env
            )
        if openvpn_run_tclient_tests:
            factory = openvpnAddTClientPreStepsToBuildFactory(
                factory, combo["configure"], make_check_env
            )

        factory = openvpnAddCheckStepsToBuildFactory(factory, combo, make_check_env)

        if openvpn_run_tclient_tests:
            factory = openvpnAddTClientPostStepsToBuildFactory(
                factory, combo["configure"], make_check_env
            )

        factory_name = getFactoryName(combo["configure"])
        factories.update(
            {
                factory_name: {
                    "factory": factory,
                    "os": "unix",
                    "types": ["openvpn", "tclient"] + combo["types"],
                    "schedulers": ["openvpn_main", "openvpn_release"],
                }
            }
        )
        del factory

# OpenVPN 2 Debian and Ubuntu packaging
for combo in packaging_config_opt_combos:
    factory = util.BuildFactory()
    factory = openvpnAddCommonUnixStepsToBuildFactory(factory, "")
    factory = openvpnAddDebianPackagingStepsToBuildFactory(factory, "")
    factory_name = getFactoryName("-package")
    factories.update(
        {
            factory_name: {
                "factory": factory,
                "os": "unix",
                "types": ["openvpn", "debian"],
                "schedulers": ["openvpn_main", "openvpn_release"],
            }
        }
    )
    del factory

# OpenVPN 2 Windows msbuild tests
if openvpn_run_windows_builds:
    for win_arch in ["amd64", "x86", "arm64", "amd64-clang", "x86-clang"]:
        factory = util.BuildFactory()
        run_tests = "arm64" not in win_arch
        win_schedulers = ["openvpn_main", "openvpn_release"]
        if "clang" in win_arch:
            win_schedulers = ["openvpn_main"]
        factory = openvpnAddCommonWindowsStepsToBuildFactory(
            factory, f"win-{win_arch}-release", run_tests, vcpkg_env
        )
        factory_name = f"msbuild-{win_arch}"
        factories.update(
            {
                factory_name: {
                    "factory": factory,
                    "os": "windows",
                    "types": ["openvpn", "msbuild"],
                    "schedulers": win_schedulers,
                }
            }
        )
        del factory

# OpenVPN 2 cmake build
factory = util.BuildFactory()
factory = openvpnAddCmakeUnixStepsToBuildFactory(factory, ccache)
factory_name = "cmake"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openvpn", "cmake"],
            "schedulers": ["openvpn_main", "openvpn_release"],
        }
    }
)
del factory

# OpenVPN 2 Android build
factory = util.BuildFactory()
factory = openvpnAddAndroidUnixStepsToBuildFactory(factory, ccache)
factory_name = "android"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openvpn", "android"],
            "schedulers": ["openvpn_main"],
        }
    }
)
del factory

# OpenVPN 2 Windows mingw build
factory = util.BuildFactory()
factory = openvpnAddCommonMingwStepsToBuildFactory(factory, ccache)
factory_name = "mingw"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openvpn", "mingw"],
            "schedulers": ["openvpn_main", "openvpn_release"],
        }
    }
)
del factory

# OpenVPN 2 clang + asan test
clang_env = {
    "CFLAGS": "-fsanitize=address,undefined -fno-sanitize-recover=all -fno-omit-frame-pointer -O2",
    "CC": "clang",
}
clang_env.update(ccache)
clang_env.update(lwip_env)

factory = util.BuildFactory()
factory = openvpnAddCommonUnixStepsToBuildFactory(factory, "", clang_env)
factory = openvpnAddUnixCompileStepsToBuildFactory(factory, "", clang_env)

clang_types = ["openvpn", "clang"]
if openvpn_run_tserver_null_tests:
    factory = openvpnAddTServerNullPreStepsToBuildFactory(factory, "", clang_env)
if openvpn_run_tclient_tests:
    factory = openvpnAddTClientPreStepsToBuildFactory(factory, "", clang_env)
    clang_types.append("tclient")

factory = openvpnAddCheckStepsToBuildFactory(factory, "", clang_env)

if openvpn_run_tclient_tests:
    factory = openvpnAddTClientPostStepsToBuildFactory(factory, "", clang_env)

factory_name = "clang"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": clang_types,
            "schedulers": ["openvpn_main", "openvpn_release"],
        }
    }
)
del factory

# openvpn3 smoketest
factory = util.BuildFactory()
factory = openvpn3AddCommonLinuxStepsToBuildFactory(factory)
factory_name = "openvpn3-smoketest"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openvpn3-smoketest"],
            "schedulers": ["openvpn3-smoketest"],
        }
    }
)
del factory

# Basic openvpn3 builds
factory = util.BuildFactory()
factory = openvpn3AddCommonLinuxStepsToBuildFactory(factory)
factory_name = "openvpn3"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openvpn3", "compile"],
            "schedulers": ["openvpn3"],
        }
    }
)
del factory

# OpenSSL openvpn3-linux smoketest builds
factory = util.BuildFactory()
factory = openvpn3LinuxAddGdbusppStepsToBuildFactory(factory, ccache)
factory = openvpn3LinuxAddCommonLinuxStepsToBuildFactory(factory, ccache)
factory_name = "openvpn3-linux-openssl-smoketest"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openssl", "openvpn3-linux-smoketest"],
            "schedulers": ["openvpn3-linux-smoketest"],
        }
    }
)
del factory

# OpenSSL openvpn3-linux builds
factory = util.BuildFactory()
factory = openvpn3LinuxAddGdbusppStepsToBuildFactory(factory, ccache)
factory = openvpn3LinuxAddCommonLinuxStepsToBuildFactory(factory, ccache)
factory_name = "openvpn3-linux-openssl"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["openssl", "openvpn3-linux", "compile"],
            "schedulers": ["openvpn3-linux"],
        }
    }
)
del factory

# ovpn-dco smoketest builds
factory = util.BuildFactory()
factory = ovpnDcoAddCommonLinuxStepsToBuildFactory(factory)
factory_name = "ovpn-dco-smoketest"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["ovpn-dco-smoketest"],
            "schedulers": ["ovpn-dco-smoketest"],
        }
    }
)
del factory

# Basic ovpn-dco builds
factory = util.BuildFactory()
factory = ovpnDcoAddCommonLinuxStepsToBuildFactory(factory)
factory_name = "ovpn-dco"
factories.update(
    {
        factory_name: {
            "factory": factory,
            "os": "unix",
            "types": ["ovpn-dco", "compile"],
            "schedulers": ["ovpn-dco"],
        }
    }
)
del factory

# Create the builders from the factories dictionary constructed above
for factory_name, factory in factories.items():
    for worker_name in worker_names:
        # Check if this factory is applicable for the worker's operating system
        if factory["os"] != worker_config.get(worker_name, "ostype"):
            continue

        # Disable builds that the worker is not capable of or which we want to
        # skip for other reasons.  These could be thought of as tags of sort.
        build_types = [
            "openvpn",
            "openvpn-smoketest",
            "openvpn3",
            "openvpn3-smoketest",
            "openvpn3-linux",
            "openvpn3-linux-smoketest",
            "openssl",
            "mbedtls",
            "debian",
            "mingw",
            "msbuild",
            "android",
            "cmake",
            "clang",
            "compile",
            "tclient",
            "ovpn-dco",
            "ovpn-dco-smoketest",
            "code-check",
            "doxygen",
        ]

        skip_build = False
        for bt in build_types:
            if bt in factory["types"]:
                if worker_config.get(worker_name, f"enable_{bt}_builds") != "true":
                    skip_build = True
        if skip_build:
            continue

        # Docker builds utilize a shared master lock in counting mode. Each Docker host
        # has its own lock with a suitable concurrent build limit.
        if worker_config.get(worker_name, "type") == "latent_docker":
            docker_host = urlparse(
                worker_config.get(worker_name, "docker_url")
            ).hostname
            locks = [docker_build_locks[docker_host].access("counting")]
        else:
            locks = None

        builder_name = f"{worker_name}-{factory_name}"

        # Pass custom build properties to the builder. This can be used to, for
        # example, to have worker-specific configure flags.
        #
        # If the value of the property would be of unset type (e.g. None or "")
        # filter them out here: that is much easier than trying to filter them
        # out in the build steps, because properties are _not_ strings, but
        # rather objects of Property type.
        #
        properties = {}

        o = worker_config.get(worker_name, "openvpn_extra_config_opts")
        if o.startswith("--"):
            properties["openvpn_extra_config_opts"] = o.split(" ")

        o = worker_config.get(worker_name, "openvpn3_linux_extra_config_opts")
        if o.startswith("--"):
            properties["openvpn3_linux_extra_config_opts"] = o.split(" ")

        cp = json.loads(worker_config.get(worker_name, "openvpn3_linux_command_prefix"))
        properties["openvpn3_linux_command_prefix"] = cp

        actual_worker_name = worker_name
        if "tclient" in factory["types"]:
            if worker_config.get(worker_name, "type") == "latent_docker":
                actual_worker_name = f"{worker_name}-tc"
            elif (
                worker_config.get(worker_name, "enable_separate_tclient_worker")
                == "true"
            ):
                actual_worker_name = f"{worker_name}-tc"

        c["builders"].append(
            util.BuilderConfig(
                name=builder_name,
                workernames=[actual_worker_name],
                factory=factory["factory"],
                properties=properties,
                locks=locks,
            )
        )

        # The schedulers need a list of applicable builder names. In practice we
        # need different set of builders for each project we are tracking.
        for scheduler in factory["schedulers"]:
            # in most cases we filter schedulers via the build type but in the
            # case of openvpn we have one build-type but two different schedulers
            # so we can filter builder names additionally by target branch
            if worker_config.get(worker_name, f"enable_{scheduler}_builds") == "true":
                builder_names[scheduler].append(builder_name)

# We need to create schedulers after the builders, because otherwise the build
# name lists are not available yet.
c["schedulers"] = []
c["services"] = []

mail_template = """\
Build status: {{ summary }}
Worker used: {{ workername }}
Build URL: {{ build_url }}

Exit codes for the build steps:
{% for step in build['steps'] %}
{{ step['name'] }}: {{ step['results'] }}
{% endfor %}

-- Buildbot
"""


def create_mails(name, mail_addresses, schedulers):
    """Send mails for builds in schedulers to specific addresses"""
    generator = reporters.BuildStatusGenerator(
        mode=("failing",),
        schedulers=schedulers,
        message_formatter=reporters.MessageFormatter(
            template_type="plain",
            template=mail_template,
            want_steps=True,
            want_logs=True,
            want_logs_content=True,
        ),
    )

    mn = reporters.MailNotifier(
        name=f"mn-{name}",
        fromaddr=fromaddr,
        sendToInterestedUsers=False,
        extraRecipients=mail_addresses,
        relayhost=relayhost,
        generators=[generator],
    )

    c["services"].append(mn)


# Only build if any of the files in the Change match this regular expression
openvpn_file_patterns = [
    "include/.*",
    "m4/.*",
    "src/.*",
    "tests/.*",
    "distro/dns-scripts/.*",
    "sample/sample-keys/.*",
    "sample/sample-config-files/loopback-.*",
    "compat.m4",
    "config.h\..*",
    "configure.ac",
    ".*Makefile.am",
    ".*CMakeLists.txt",
    "version.m4",
    ".*\.rst",
    "doc/Makefile.am",
]

openvpn_filter_fn = "^(" + "|".join(openvpn_file_patterns) + ")$"


openvpn_branches = {"main": openvpn_main_branch, "release": openvpn_release_branch}
for branch_type, branch_name in openvpn_branches.items():
    # Ensure that in OpenVPN 2 we run smoke tests first and only if those pass run
    # the full test suite
    openvpn_smoketest_scheduler = schedulers.SingleBranchScheduler(
        name=f"openvpn-smoketest-{branch_type}",
        change_filter=util.ChangeFilter(
            branch=branch_name,
            project="openvpn",
            filter_fn=lambda c: any([re.search(openvpn_filter_fn, f) for f in c.files]),
        ),
        treeStableTimer=openvpn_tree_stable_timer,
        builderNames=builder_names["openvpn-smoketest"],
        properties={"owner": openvpn_owner},
    )

    openvpn_full_scheduler = schedulers.Dependent(
        name=f"openvpn-full-{branch_type}",
        upstream=openvpn_smoketest_scheduler,
        builderNames=builder_names[f"openvpn_{branch_type}"],
        properties={"owner": openvpn_owner},
    )

    openvpn_gerrit_smoketest_scheduler = schedulers.SingleBranchScheduler(
        name=f"openvpn-gerrit-smoketest-{branch_type}",
        change_filter=util.ChangeFilter(
            repository_re=".*gerrit.*",
            project="openvpn",
            filter_fn=lambda c: any([re.search(openvpn_filter_fn, f) for f in c.files]),
            property_eq={
                "event.patchSet.uploader.name": verified_authors_list,
                "target_branch": branch_name,
            },
            property_not_eq={
                "event.change.status": "MERGED",
            },
        ),
        priority=1,
        treeStableTimer=openvpn_tree_stable_timer,
        builderNames=builder_names["openvpn-smoketest"],
        properties={"owner": openvpn_owner},
    )

    openvpn_gerrit_full_scheduler = schedulers.Dependent(
        name=f"openvpn-gerrit-full-{branch_type}",
        upstream=openvpn_gerrit_smoketest_scheduler,
        builderNames=builder_names[f"openvpn_{branch_type}"],
        properties={"owner": openvpn_owner},
    )

    c["schedulers"].append(openvpn_smoketest_scheduler)
    c["schedulers"].append(openvpn_full_scheduler)
    c["schedulers"].append(openvpn_gerrit_smoketest_scheduler)
    c["schedulers"].append(openvpn_gerrit_full_scheduler)
    create_mails(
        f"openvpn-{branch_type}",
        extra_recipients + [openvpn_owner],
        [
            openvpn_smoketest_scheduler.name,
            openvpn_full_scheduler.name,
            openvpn_gerrit_smoketest_scheduler.name,
            openvpn_gerrit_full_scheduler.name,
        ],
    )

openvpn3_smoketest_scheduler = schedulers.SingleBranchScheduler(
    name="openvpn3-smoketest",
    change_filter=util.ChangeFilter(branch=openvpn3_branch, project="openvpn3"),
    treeStableTimer=openvpn3_tree_stable_timer,
    builderNames=builder_names["openvpn3-smoketest"],
    properties={"owner": openvpn3_owner},
)

openvpn3_default_scheduler = schedulers.Dependent(
    name="openvpn3-default",
    upstream=openvpn3_smoketest_scheduler,
    builderNames=builder_names["openvpn3"],
    properties={"owner": openvpn3_owner},
)

c["schedulers"].append(openvpn3_smoketest_scheduler)
c["schedulers"].append(openvpn3_default_scheduler)
create_mails(
    "openvpn3",
    extra_recipients + [openvpn3_owner],
    [openvpn3_smoketest_scheduler.name, openvpn3_default_scheduler.name],
)

openvpn3_linux_smoketest_scheduler = schedulers.SingleBranchScheduler(
    name="openvpn3-linux-smoketest",
    change_filter=util.ChangeFilter(
        branch=openvpn3_linux_branch, project="openvpn3-linux"
    ),
    treeStableTimer=openvpn3_linux_tree_stable_timer,
    builderNames=builder_names["openvpn3-linux-smoketest"],
    properties={"owner": openvpn3_linux_owner},
)

openvpn3_linux_default_scheduler = schedulers.Dependent(
    name="openvpn3-linux-default",
    upstream=openvpn3_linux_smoketest_scheduler,
    builderNames=builder_names["openvpn3-linux"],
    properties={"owner": openvpn3_linux_owner},
)

c["schedulers"].append(openvpn3_linux_smoketest_scheduler)
c["schedulers"].append(openvpn3_linux_default_scheduler)
create_mails(
    "openvpn3_linux",
    extra_recipients + [openvpn3_linux_owner],
    [openvpn3_linux_smoketest_scheduler.name, openvpn3_linux_default_scheduler.name],
)

ovpn_dco_smoketest_scheduler = schedulers.SingleBranchScheduler(
    name="ovpn-dco-smoketest",
    change_filter=util.ChangeFilter(branch=ovpn_dco_branch, project="ovpn-dco"),
    treeStableTimer=ovpn_dco_tree_stable_timer,
    builderNames=builder_names["ovpn-dco-smoketest"],
    properties={"owner": ovpn_dco_owner},
)

ovpn_dco_default_scheduler = schedulers.Dependent(
    name="ovpn-dco-default",
    upstream=ovpn_dco_smoketest_scheduler,
    builderNames=builder_names["ovpn-dco"],
    properties={"owner": ovpn_dco_owner},
)

c["schedulers"].append(ovpn_dco_smoketest_scheduler)
c["schedulers"].append(ovpn_dco_default_scheduler)
create_mails(
    "ovpn_dco",
    extra_recipients + [ovpn_dco_owner],
    [ovpn_dco_smoketest_scheduler.name, ovpn_dco_default_scheduler.name],
)

c["schedulers"].append(
    schedulers.ForceScheduler(
        name="openvpn-main-force",
        builderNames=builder_names["openvpn_main"],
        codebases=[
            util.CodebaseParameter(
                "",
                label="Commit",
                branch=util.StringParameter(name="branch", default=openvpn_main_branch),
                revision=util.StringParameter(name="revision", default=""),
                repository=util.StringParameter(
                    name="repository", default=openvpn_repo_url
                ),
                project=util.FixedParameter(name="project", default="openvpn"),
            ),
        ],
    )
)
c["schedulers"].append(
    schedulers.ForceScheduler(
        name="openvpn-release-force",
        builderNames=builder_names["openvpn_release"],
        codebases=[
            util.CodebaseParameter(
                "",
                label="Commit",
                branch=util.StringParameter(
                    name="branch", default=openvpn_release_branch
                ),
                revision=util.StringParameter(name="revision", default=""),
                repository=util.StringParameter(
                    name="repository", default=openvpn_repo_url
                ),
                project=util.FixedParameter(name="project", default="openvpn"),
            ),
        ],
    )
)

c["schedulers"].append(
    schedulers.ForceScheduler(
        name="openvpn3-force",
        builderNames=builder_names["openvpn3"],
        codebases=[
            util.CodebaseParameter(
                "",
                label="Commit",
                branch=util.StringParameter(name="branch", default=openvpn3_branch),
                revision=util.StringParameter(name="revision", default=""),
                repository=util.StringParameter(
                    name="repository", default=openvpn3_repo_url
                ),
                project=util.FixedParameter(name="project", default="openvpn3"),
            ),
        ],
    )
)

c["schedulers"].append(
    schedulers.ForceScheduler(
        name="openvpn3-linux-force",
        builderNames=builder_names["openvpn3-linux"],
        codebases=[
            util.CodebaseParameter(
                "",
                label="Commit",
                branch=util.StringParameter(
                    name="branch", default=openvpn3_linux_branch
                ),
                revision=util.StringParameter(name="revision", default=""),
                repository=util.StringParameter(
                    name="repository", default=openvpn3_linux_repo_url
                ),
                project=util.FixedParameter(name="project", default="openvpn3-linux"),
            ),
        ],
    )
)

c["schedulers"].append(
    schedulers.ForceScheduler(
        name="ovpn-dco-force",
        builderNames=builder_names["ovpn-dco"],
        codebases=[
            util.CodebaseParameter(
                "",
                label="Commit",
                branch=util.StringParameter(name="branch", default=ovpn_dco_branch),
                revision=util.StringParameter(name="revision", default=""),
                repository=util.StringParameter(
                    name="repository", default=ovpn_dco_repo_url
                ),
                project=util.FixedParameter(name="project", default="ovpn-dco"),
            ),
        ],
    )
)

missing_mn = reporters.MailNotifier(
    fromaddr=fromaddr,
    sendToInterestedUsers=False,
    extraRecipients=extra_recipients,
    relayhost=relayhost,
    generators=[reporters.WorkerMissingGenerator()],
)

c["services"].append(missing_mn)

c["services"].append(
    GerritChecks(
        baseURL=gerrit_repo_url, auth=("buildbot", gerrit_user_password), verbose=True
    )
)

# If enabled, cancel previous builds if a new commit is pushed to a non-master
# branch. This is very useful on development buildbot systems which may start
# building obsolete commits on startup.
if cancel_old_builds:
    obc = util.OldBuildCanceller(
        "build_canceller",
        filters=[
            (builder_names["openvpn_main"], SourceStampFilter(branch_not_eq="master"))
        ],
    )

    c["services"].append(obc)

c["title"] = "OpenVPN buildbot"
c["titleURL"] = title_url
c["buildbotURL"] = buildbot_url
c["www"] = {
    "port": r"tcp:interface=\:\:0:port=8010",
    "plugins": {
        "waterfall_view": True,
    },
}

c["db"] = {
    "db_url": database,
}

c["configurators"] = [
    util.JanitorConfigurator(
        logHorizon=timedelta(weeks=12),
        hour=3,
    )
]
